<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>My Diary</title>
    <script src="https://cdn.tailwindcss.com"></script>
  </head>
  <body class="min-h-screen bg-gradient-to-r from-purple-500 to-pink-500 p-8">
    <div class="max-w-3xl mx-auto bg-white rounded-lg shadow-lg p-6">

      <h2 class="text-2xl font-bold underline bg-yellow-400 text-center py-2 mb-4">Date: 17-10-2024</h2>
      <h1 class="text-3xl font-semibold text-center mb-6">Diary on Computer Organization and Architecture (COA)</h1>
            <div class="space-y-8">
                    <h3 class="text-xl font-semibold  mb-2">Date: October 2024 <u>Topic: Introduction to COA</u></h3>
                    <p>
                    I’ve just started diving into the subject of Computer Organization and
                    Architecture (COA), which feels like opening a window into the brain of
                    computers! It’s all about how computers are built, how they work at a
                    fundamental level, and the principles behind their operation. Before we
                    start with modern systems, I want to understand how everything began.
                    </p>
                    
                    <h3 class="text-xl font-semibold mb-2">Date: June 1837 <u> Charles Babbage: The Father of Computers</u></h3>
                    <p>
                    Today, I read about Charles Babbage, often referred to as the “father of
                    computers.” In 1837, he designed the first mechanical computer called the
                    Analytical Engine. Though it was never built in his lifetime, the concept
                    was revolutionary. It featured key elements we still study today in COA,
                    such as an arithmetic logic unit (ALU), memory, and even control flow.
                    Imagine how ahead of his time he was! This laid the foundation for what
                    would become modern-day computer architecture.
                    </p>
                    
                    <h3 class="text-xl font-semibold  mb-2">Date: 1940s <u> The Birth of Modern Computing</u></h3>
                    <p>
                    Fast forward a century to the 1940s, during World War II, computers became
                    practical. ENIAC (Electronic Numerical Integrator and Computer), developed
                    by John Mauchly and J. Presper Eckert, was one of the first
                    general-purpose electronic digital computers. It was built in 1945 and
                    could perform 5,000 additions per second! However, ENIAC used vacuum
                    tubes—big, clunky devices that took up entire rooms and were prone to
                    failure.
                    </p>
                    <p>
                    This era was all about the Von Neumann Architecture, named after
                    mathematician John von Neumann, who proposed a new design where data and
                    instructions would be stored in the same memory. This concept is now a
                    fundamental building block of COA.
                    </p>
                    
                    <h3 class="text-xl font-semibold mb-2">Date: 1950s <u>The Rise of Transistors</u></h3>
                    <p>
                    The 1950s brought a game-changing development: the transistor! The vacuum
                    tubes that were common in early computers like ENIAC were replaced by
                    transistors, which were smaller, faster, and more reliable. The UNIVAC
                    (Universal Automatic Computer), built in 1951, was the first commercially
                    available computer. I found it fascinating that this was the beginning of
                    computers being used not just for scientific purposes but for businesses
                    as well.
                    </p>

                    <h3 class="text-xl font-semibold mb-2">Date: 1960s <u>Integrated Circuits and Moore’s Law</u></h3>
                    <p>
                    The 1960s is one of my favorite decades to read about! It saw the
                    invention of integrated circuits (ICs), which were essentially many
                    transistors packed onto a single chip. This led to much smaller and faster
                    computers, pushing the boundaries of what was possible.
                    </p>
                    <p>
                    This decade also gave birth to Moore’s Law, named after Gordon Moore, the
                    co-founder of Intel. He predicted that the number of transistors on a chip
                    would double approximately every two years. Reading this, I wonder—what
                    does the future hold if this trend continues?
                    </p>
                    
                    <h3 class="text-xl font-semibold mb-2">Date: 1970s <u> The Microprocessor Revolution</u></h3>
                    <p>
                    Ah, the 1970s! This is when everything truly started accelerating. Intel
                    introduced the 4004 microprocessor in 1971. It was the first commercially
                    available microprocessor and marked the beginning of a new era. A single
                    chip could now perform all the functions of a CPU! The term
                    "microprocessor" itself became synonymous with computing power. From here
                    on, computers became personal, affordable, and more ubiquitous in daily
                    life.
                    </p>
                    <p>
                    I also discovered that Harvard architecture was developed in this period,
                    separating the storage and signal pathways for instructions and data,
                    which helped improve performance. This is still used in specialized
                    systems today, like embedded systems.
                    </p>
                    
                    <h3 class="text-xl font-semibold  mb-2">Date: 1980s <u>Rise of Personal Computers and RISC</u></h3>
                    <p>
                    This is when things start looking a lot like today’s world. The 1980s saw
                    the rise of personal computers (PCs). Companies like Apple, IBM, and
                    Microsoft started bringing computers into homes and offices.
                    </p>
                    <p>
                    Meanwhile, a significant concept in COA developed: RISC (Reduced
                    Instruction Set Computer). Introduced in the early 1980s, RISC
                    architectures simplified instructions executed by the CPU, which led to
                    faster processing. I find it intriguing that while most people were
                    focused on making user-friendly software, COA scholars were focusing on
                    making machines faster and more efficient under the hood.
                    </p>
                    
                    <h3 class="text-xl font-semibold  mb-2">Date: 1990s <u>Superscalar Architecture</u></h3>
                    <p>
                    Now it gets technical and super interesting. The 1990s introduced
                    superscalar architecture—a method allowing CPUs to execute more than one
                    instruction per clock cycle. This was a big leap in performance. I learned
                    that the Pentium processors from Intel, which powered a lot of PCs in this
                    decade, used this architecture.
                    </p>
                    <p>
                    By this point, COA was more about optimizing performance, power
                    consumption, and reducing the physical size of processors while increasing
                    their speed
                    </p>
                    
                    <h3 class="text-xl font-semibold  mb-2">Date: 2000s <u>Multicore Processors</u></h3>
                    <p>
                    Entering the 21st century, I learned about the multicore revolution.
                    Instead of making a single core faster, chip manufacturers like Intel and
                    AMD started putting multiple cores on a single chip. This allowed
                    computers to handle multiple processes simultaneously, dramatically
                    improving multitasking and performance. The concept of
                    parallelism—performing multiple tasks at once—is now a key focus of COA.
                    </p>
                    
                    <h3 class="text-xl font-semibold  mb-2">Date: 2010s <u>Rise of GPUs and Cloud Computing</u></h3>
                    <p>
                    In the 2010s, I realized that while CPUs were evolving, GPUs (Graphics
                    Processing Units) also became critical, especially for tasks like machine
                    learning, gaming, and cryptocurrency mining. GPUs are specialized
                    processors designed to handle massive amounts of parallel operations, and
                    they’ve become crucial in various fields. This decade also saw the massive
                    growth of cloud computing, changing the way architecture is approached.
                    Instead of focusing on single machines, we now think of distributed
                    architectures, where servers handle data and computation in the cloud.
                    </p>
                    
                    <h3 class="text-xl font-semibold  mb-2">Date: 2020s <u>Quantum Computing and the Future</u></h3>
                    <p>
                    Now, we’re in the age of quantum computing. Though we’re still in the
                    early stages, I’m fascinated by how quantum computers promise to
                    revolutionize computation by solving complex problems that traditional
                    computers struggle with. Companies like IBM, Google, and Microsoft are
                    leading the charge in this space, and COA may change dramatically as
                    quantum machines develop.
                    </p>

                    
                    <h4 class="text-lg font-bold mt-8">Personal Reflection:</h4>
                    <p>
                    Writing this diary has made me realize that Computer Organization and
                    Architecture is more than just hardware and circuits—it’s the story of
                    human ingenuity, pushing the boundaries of what we can do with technology.
                    Each decade brought advancements that shaped our world today, and I’m
                    excited to see what the future holds. I’ll keep updating this diary as I
                    learn more in class.
                    </p>

                    <h4 class="text-lg font-bold mt-8">PDF TO READ </h4>
                    <a href="My Diary.pdf" download="MyFile.pdf">Download PDF</a>
            </div>   
                </div>  
        </body>
</html>
